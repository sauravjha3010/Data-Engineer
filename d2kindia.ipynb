{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4vC9RniIcRL",
        "outputId": "1280c214-e365-4cb0-da0b-e457f89285ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=3ea57ceadd61baa706d0493657eecf7b4f273c74b1630a8e78bef959e382cb17\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "!pip install requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NYC Taxi Data Analysis\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "chXzda1SIhyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Base URL for the datasets\n",
        "base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n",
        "\n",
        "# Directory to save downloaded files\n",
        "download_dir = \"NYC_Taxi_2019\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# Months and types of datasets\n",
        "months = [\n",
        "    \"2019-01\", \"2019-02\", \"2019-03\", \"2019-04\", \"2019-05\", \"2019-06\",\n",
        "    \"2019-07\", \"2019-08\", \"2019-09\", \"2019-10\", \"2019-11\", \"2019-12\"\n",
        "]\n",
        "dataset_types = [\n",
        "    \"yellow\", \"green\", \"fhv\", \"high_volume_fhv\"\n",
        "]\n",
        "\n",
        "# Function to download files with retry mechanism\n",
        "def download_file(url, local_path):\n",
        "    retries = 3\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            with open(local_path, 'wb') as f:\n",
        "                for chunk in tqdm(response.iter_content(chunk_size=1024), desc=f\"Downloading {local_path}\"):\n",
        "                    if chunk:  # filter out keep-alive new chunks\n",
        "                        f.write(chunk)\n",
        "            break\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error: {e}. Retrying... ({i+1}/{retries})\")\n",
        "            time.sleep(5)\n",
        "    else:\n",
        "        print(f\"Failed to download {local_path} after {retries} retries.\")\n",
        "\n",
        "# Download all datasets\n",
        "for month in months:\n",
        "    for dataset_type in dataset_types:\n",
        "        filename = f\"{dataset_type}_tripdata_{month}.parquet\"\n",
        "        url = f\"{base_url}{filename}\"\n",
        "        local_path = os.path.join(download_dir, filename)\n",
        "        print(f\"Downloading {url}...\")\n",
        "        download_file(url, local_path)\n",
        "\n",
        "print(\"Download complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFBT4oCpJGE1",
        "outputId": "18c3eb6e-c93b-4bd7-ad59-47363bc8b4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-01.parquet: 107852it [00:01, 70685.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-01.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-01.parquet: 10813it [00:00, 41382.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-01.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-01.parquet: 205769it [00:05, 38618.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-01.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-01.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-01.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-01.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-01.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-02.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-02.parquet: 100934it [00:01, 69931.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-02.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-02.parquet: 10075it [00:00, 47332.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-02.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-02.parquet: 14925it [00:00, 40924.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-02.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-02.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-02.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-02.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-02.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-03.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-03.parquet: 113299it [00:03, 32385.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-03.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-03.parquet: 10509it [00:00, 37686.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-03.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-03.parquet: 13259it [00:00, 46025.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-03.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-03.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-03.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-03.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-03.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-04.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-04.parquet: 107558it [00:01, 65981.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-04.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-04.parquet: 9193it [00:00, 42725.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-04.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-04.parquet: 16675it [00:00, 37657.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-04.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-04.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-04.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-04.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-04.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-05.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-05.parquet: 108867it [00:01, 68817.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-05.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-05.parquet: 8839it [00:00, 35910.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-05.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-05.parquet: 17779it [00:00, 49667.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-05.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-05.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-05.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-05.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-05.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-06.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-06.parquet: 100492it [00:01, 65605.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-06.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-06.parquet: 8349it [00:00, 29610.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-06.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-06.parquet: 17353it [00:00, 33415.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-06.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-06.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-06.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-06.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-06.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-07.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-07.parquet: 91678it [00:01, 68424.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-07.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-07.parquet: 7736it [00:00, 39996.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-07.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-07.parquet: 16909it [00:00, 50660.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-07.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-07.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-07.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-07.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-07.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-08.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-08.parquet: 87891it [00:01, 49241.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-08.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-08.parquet: 7378it [00:00, 21333.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-08.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-08.parquet: 16904it [00:00, 44270.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-08.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-08.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-08.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-08.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-08.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-09.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-09.parquet: 94835it [00:01, 67892.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-09.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-09.parquet: 7512it [00:00, 43165.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-09.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-09.parquet: 11969it [00:00, 46832.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-09.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-09.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-09.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-09.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-09.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-10.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-10.parquet: 103803it [00:02, 42952.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-10.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-10.parquet: 7681it [00:00, 30303.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-10.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-10.parquet: 17915it [00:00, 51358.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-10.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-10.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-10.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-10.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-10.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-11.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-11.parquet: 98509it [00:02, 40216.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-11.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-11.parquet: 7407it [00:00, 30321.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-11.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-11.parquet: 17737it [00:00, 49584.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-11.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-11.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-11.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-11.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-11.parquet after 3 retries.\n",
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-12.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/yellow_tripdata_2019-12.parquet: 98677it [00:02, 49080.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-12.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/green_tripdata_2019-12.parquet: 7347it [00:00, 38875.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-12.parquet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading NYC_Taxi_2019/fhv_tripdata_2019-12.parquet: 19179it [00:00, 50542.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-12.parquet...\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-12.parquet. Retrying... (1/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-12.parquet. Retrying... (2/3)\n",
            "Error: 403 Client Error: Forbidden for url: https://d37ci6vzurychx.cloudfront.net/trip-data/high_volume_fhv_tripdata_2019-12.parquet. Retrying... (3/3)\n",
            "Failed to download NYC_Taxi_2019/high_volume_fhv_tripdata_2019-12.parquet after 3 retries.\n",
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example file path (adjust as needed)\n",
        "file_path = \"NYC_Taxi_2019/yellow_tripdata_2019-01.parquet\"\n",
        "\n",
        "# Load the data into a Pandas DataFrame\n",
        "df = pd.read_parquet(file_path)\n",
        "\n",
        "# Drop rows with missing or null values in critical columns\n",
        "df_clean = df.dropna(subset=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount'])\n",
        "\n",
        "# Filter out trips with non-positive distance or fare\n",
        "df_clean = df_clean[(df_clean['trip_distance'] > 0) & (df_clean['fare_amount'] > 0)]\n"
      ],
      "metadata": {
        "id": "hMJ-g7HEJrbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert datetime columns to pandas datetime type\n",
        "df_clean['tpep_pickup_datetime'] = pd.to_datetime(df_clean['tpep_pickup_datetime'])\n",
        "df_clean['tpep_dropoff_datetime'] = pd.to_datetime(df_clean['tpep_dropoff_datetime'])\n",
        "\n",
        "# Calculate trip duration in minutes\n",
        "df_clean['trip_duration'] = (df_clean['tpep_dropoff_datetime'] - df_clean['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "\n",
        "# Calculate average speed in miles per hour\n",
        "df_clean['average_speed'] = df_clean['trip_distance'] / (df_clean['trip_duration'] / 60)\n"
      ],
      "metadata": {
        "id": "TIlqZP86LZmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the day from the pickup datetime\n",
        "df_clean['day'] = df_clean['tpep_pickup_datetime'].dt.date\n",
        "\n",
        "# Group by day to get total trips and average fare\n",
        "daily_aggregates = df_clean.groupby('day').agg(\n",
        "    total_trips=pd.NamedAgg(column='VendorID', aggfunc='count'),\n",
        "    average_fare=pd.NamedAgg(column='fare_amount', aggfunc='mean')\n",
        ").reset_index()\n"
      ],
      "metadata": {
        "id": "B1MLA8xhMvOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Create a new SQLite database (or connect to existing)\n",
        "conn = sqlite3.connect('nyc_taxi_data.db')\n",
        "\n",
        "# Define the schema for the trips table\n",
        "create_table_query = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS trips (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    VendorID INTEGER,\n",
        "    tpep_pickup_datetime TEXT,\n",
        "    tpep_dropoff_datetime TEXT,\n",
        "    passenger_count INTEGER,\n",
        "    trip_distance REAL,\n",
        "    fare_amount REAL,\n",
        "    trip_duration REAL,\n",
        "    average_speed REAL,\n",
        "    day DATE\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query to create the table\n",
        "conn.execute(create_table_query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKV1i9QQM0Ec",
        "outputId": "150248a1-5b6a-4209-ee9b-04581c10439c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sqlite3.Cursor at 0x7d4afbefac40>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Example file path (adjust as needed)\n",
        "file_path = \"NYC_Taxi_2019/yellow_tripdata_2019-01.parquet\"\n",
        "\n",
        "# Load the data into a Pandas DataFrame\n",
        "df = pd.read_parquet(file_path)\n",
        "\n",
        "# Clean the data\n",
        "df_clean = df.dropna(subset=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount'])\n",
        "df_clean = df_clean[(df_clean['trip_distance'] > 0) & (df_clean['fare_amount'] > 0)]\n",
        "\n",
        "# Derive new columns\n",
        "df_clean['tpep_pickup_datetime'] = pd.to_datetime(df_clean['tpep_pickup_datetime'])\n",
        "df_clean['tpep_dropoff_datetime'] = pd.to_datetime(df_clean['tpep_dropoff_datetime'])\n",
        "df_clean['trip_duration'] = (df_clean['tpep_dropoff_datetime'] - df_clean['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
        "df_clean['average_speed'] = df_clean['trip_distance'] / (df_clean['trip_duration'] / 60)\n",
        "\n",
        "# Connect to SQLite database\n",
        "conn = sqlite3.connect('nyc_taxi_data.db')\n",
        "\n",
        "# Define the schema with all necessary columns\n",
        "create_table_query = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS trips (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    VendorID INTEGER,\n",
        "    tpep_pickup_datetime TEXT,\n",
        "    tpep_dropoff_datetime TEXT,\n",
        "    passenger_count INTEGER,\n",
        "    trip_distance REAL,\n",
        "    RatecodeID INTEGER,\n",
        "    store_and_fwd_flag TEXT,\n",
        "    PULocationID INTEGER,\n",
        "    DOLocationID INTEGER,\n",
        "    payment_type INTEGER,\n",
        "    fare_amount REAL,\n",
        "    extra REAL,\n",
        "    mta_tax REAL,\n",
        "    tip_amount REAL,\n",
        "    tolls_amount REAL,\n",
        "    improvement_surcharge REAL,\n",
        "    total_amount REAL,\n",
        "    congestion_surcharge REAL,\n",
        "    trip_duration REAL,\n",
        "    average_speed REAL,\n",
        "    day DATE\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# Create table\n",
        "conn.execute(create_table_query)\n",
        "\n",
        "# Load data into the database\n",
        "df_clean.to_sql('trips', conn, if_exists='append', index=False)\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "sEK0jHaMM3M9",
        "outputId": "65213734-aa66-4e2d-9765-9f4b697348aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-94b1d2357f01>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_clean['tpep_pickup_datetime'] = pd.to_datetime(df_clean['tpep_pickup_datetime'])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "table trips has no column named RatecodeID",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-94b1d2357f01>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Load data into the database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trips'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Close the connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2876\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2878\u001b[0;31m         return sql.to_sql(\n\u001b[0m\u001b[1;32m   2879\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2880\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpandasSQL_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_transaction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpandas_sql\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return pandas_sql.to_sql(\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m   2377\u001b[0m         )\n\u001b[1;32m   2378\u001b[0m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhas_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, chunksize, method)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0mchunk_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m                 \u001b[0mnum_inserted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexec_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m                 \u001b[0;31m# GH 46891\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_inserted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36m_execute_insert\u001b[0;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[1;32m   2073\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m         \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: table trips has no column named RatecodeID"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT strftime('%H', tpep_pickup_datetime) AS hour, COUNT(*) AS trip_count\n",
        "FROM trips\n",
        "GROUP BY hour\n",
        "ORDER BY trip_count DESC;\n"
      ],
      "metadata": {
        "id": "JhfENw_OM6FS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}